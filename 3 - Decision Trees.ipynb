{
 "metadata": {
  "name": "",
  "signature": "sha256:83d5cde1706ff1059e42d35e5a390cfea8b5e0c5991dc5d3216770f930c32029"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decision Trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pros: \n",
      "* Computationally cheap to use\n",
      "* Easy for humans to understand learned results*\n",
      "* Missing values OK\n",
      "* Can deal with irrelevant features\n",
      "\n",
      "Cons: \n",
      "* Prone to overfitting\n",
      "\n",
      "Works with: \n",
      "* Numeric values\n",
      "* Nominal values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calcShannonEnt(dataSet):\n",
      "    # create dictionary of all possible classes\n",
      "    numEntries = len(dataSet)\n",
      "    labelCounts = {}\n",
      "    for featVec in dataSet:\n",
      "        currentLabel = featVec[-1]\n",
      "        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n",
      "        labelCounts[currentLabel] += 1\n",
      "\n",
      "    # calculate entropy using Shannon's formula\n",
      "    shannonEnt = 0.0\n",
      "    for key in labelCounts:\n",
      "        prob = float(labelCounts[key]) / numEntries\n",
      "        shannonEnt -= prob * log(prob, 2)\n",
      "    return shannonEnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createDataSet():\n",
      "    dataSet = [[1, 1, 'yes'],\n",
      "               [1, 1, 'yes'],\n",
      "               [1, 0, 'no'],\n",
      "               [0, 1, 'no'],\n",
      "               [0, 1, 'no']]\n",
      "    labels = ['no surfacing','flippers']\n",
      "    return dataSet, labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myDat, labels = createDataSet()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calcShannonEnt(myDat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.9709505944546686"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The higher the entropy, the more mixed up the data is. Let\u2019s make the data a little messier and see how the entropy changes. We\u2019ll add a third class, which is called maybe, and see how the entropy changes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myDat[0][-1]='maybe'\n",
      "myDat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calcShannonEnt(myDat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "1.3709505944546687"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def splitDataSet(dataSet, axis, value):\n",
      "    retDataSet = []\n",
      "    for featVec in dataSet:\n",
      "        if featVec[axis] == value:\n",
      "            # remove the axis from the data, since it's already splitted on it\n",
      "            reducedFeatVec = featVec[:axis]\n",
      "            reducedFeatVec.extend(featVec[axis+1:])\n",
      "            retDataSet.append(reducedFeatVec)\n",
      "    return retDataSet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myDat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "splitDataSet(myDat,1,0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[[1, 'no']]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chooseBestFeatureToSplit(dataSet):\n",
      "    \"\"\"\n",
      "    Splitting the dataSet on different features, and deciding which feature is best to split\n",
      "    the data on.\n",
      "    \"\"\"\n",
      "    numFeatures = len(dataSet[0]) - 1  # because the last column is class type\n",
      "    baseEntropy = calcShannonEnt(dataSet)\n",
      "    bestInfoGain = 0.0; bestFeature = -1\n",
      "    \n",
      "    for i in range(numFeatures):\n",
      "        featList = [example[i] for example in dataSet]\n",
      "        uniqueVals = set(featList)\n",
      "        newEntropy = 0.0\n",
      "        for value in uniqueVals:\n",
      "            subDataSet = splitDataSet(dataSet, i, value)\n",
      "            prob = len(subDataSet) / float(len(dataSet))\n",
      "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
      "        infoGain = baseEntropy - newEntropy\n",
      "        if (infoGain > bestInfoGain):\n",
      "            bestInfoGain = infoGain\n",
      "            bestFeature = i\n",
      "    return bestFeature"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chooseBestFeatureToSplit(myDat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def majorityCnt(classList):\n",
      "    \"\"\"\n",
      "    This function takes a list of class names and then creates a dictionary \n",
      "    whose keys are the unique values in classList, and the object of the \n",
      "    dictionary is the frequency of occurrence of each class label from classList. \n",
      "    Finally, you use the operator to sort the dictionary by the keys and return \n",
      "    the class that occurs with the greatest frequency.\n",
      "    \"\"\"\n",
      "    classCount={}\n",
      "    for vote in classList:\n",
      "        if vote not in classCount.keys(): classCount[vote] = 0\n",
      "        classCount[vote] += 1\n",
      "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "    return sortedClassCount[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createTree(dataSet, labels):\n",
      "    \"\"\"\n",
      "    Recursively finding the best feature to split on, and split until\n",
      "    a leaf is pure, or no feature is left.\n",
      "    Python dictionary will be used to store the tree, so you end up with \n",
      "    a lot of nested dictionaries representing the tree.\n",
      "    \"\"\"\n",
      "    \n",
      "    # if all class labels are the same, then you return this label\n",
      "    classList = [example[-1] for example in dataSet]\n",
      "    if classList.count(classList[0]) == len(classList):\n",
      "        return classList[0]\n",
      "    \n",
      "    # stops when there are no more features to split on\n",
      "    if len(dataSet[0]) == 1:\n",
      "        return majorityCnt(classList)\n",
      "    \n",
      "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
      "    bestFeatLabel = labels[bestFeat]\n",
      "    myTree = {bestFeatLabel:{}}\n",
      "    \n",
      "    del(labels[bestFeat])\n",
      "    \n",
      "    featValues = [example[bestFeat] for example in dataSet]\n",
      "    uniqueVals = set(featValues)\n",
      "    for value in uniqueVals:\n",
      "        subLabels = labels[:]\n",
      "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
      "    \n",
      "    return myTree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myDat,labels = createDataSet()\n",
      "myTree = createTree(myDat,labels)\n",
      "myTree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using the tree for classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(inputTree, featLabels, testVec):\n",
      "    \"\"\"\n",
      "    Recursively travel the tree, comparing the values in 'testVec' \n",
      "    to the values in the tree. If you reach a leaf node, you\u2019ve made \n",
      "    your classification and it\u2019s time to exit.\n",
      "    \n",
      "    'featLables' is used to translate the index and label name\n",
      "    \"\"\"\n",
      "    firstStr = inputTree.keys()[0]\n",
      "    secondDict = inputTree[firstStr]\n",
      "    featIndex = featLabels.index(firstStr)\n",
      "    for key in secondDict.keys():\n",
      "        if testVec[featIndex] == key:\n",
      "            if type(secondDict[key]).__name__=='dict':\n",
      "                classLabel = classify(secondDict[key],featLabels,testVec)\n",
      "            else:\n",
      "                classLabel = secondDict[key]\n",
      "                \n",
      "    return classLabel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myDat,labels = createDataSet()\n",
      "classify(myTree,labels,[1,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "'no'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Persisting the decision tree using Pickle"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It would be a waste of time to build the tree every time you wanted to make a classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def storeTree(inputTree,filename):\n",
      "    import pickle\n",
      "    fw = open(filename,'w')\n",
      "    pickle.dump(inputTree,fw)\n",
      "    fw.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grabTree(filename):\n",
      "    import pickle\n",
      "    fr = open(filename)\n",
      "    return pickle.load(fr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "storeTree(myTree,'classifierStorage.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grabTree('classifierStorage.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using decision trees to predict contact lens type"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fr = open('machinelearninginaction/Ch03/lenses.txt')\n",
      "lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
      "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
      "lensesTree = createTree(lenses, lensesLabels)\n",
      "lensesTree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "{'tearRate': {'normal': {'astigmatic': {'no': {'age': {'pre': 'soft',\n",
        "      'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}},\n",
        "      'young': 'soft'}},\n",
        "    'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses',\n",
        "        'presbyopic': 'no lenses',\n",
        "        'young': 'hard'}},\n",
        "      'myope': 'hard'}}}},\n",
        "  'reduced': 'no lenses'}}"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In chapter 9 we\u2019ll also investigate another decision tree algorithm called CART. The algorithm we used in this chapter, ID3, is good but not the best. ID3 can\u2019t handle numeric values. We could use continuous values by quantizing them into discrete bins, but ID3 suffers from other problems if we have too many splits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}